{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaASD1/C-basic-questions/blob/main/Digit_classifier_updated_original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf2XFhtLrujM",
        "outputId": "ada6c139-9773-42e5-f41e-842f15f44a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-396e4441eb54>:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.autonotebook import tqdm\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minst = tf.keras.datasets.mnist"
      ],
      "metadata": {
        "id": "FzNAaBSErz_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train,y_train),(x_test,y_test) = minst.load_data()\n",
        "# plt.imshow(x_train[0])\n",
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvRCsQMrr2ba",
        "outputId": "6da85967-0f8c-4854-c774-6fe5514f044c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape((x_train.shape[0],28,28,1))/255\n",
        "\n",
        "x_test = x_test.reshape((x_test.shape[0],28,28,1))/255\n"
      ],
      "metadata": {
        "id": "853NvlP2sCe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualUnit(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, activation=tf.keras.layers.LeakyReLU(alpha=0.2), **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "                            tf.keras.layers.Conv2D(filters, kernel_size=3, strides=strides, padding=\"SAME\"),\n",
        "                            tf.keras.layers.BatchNormalization()(),\n",
        "                            self.activation,\n",
        "                            tf.keras.layers.Conv2D(filters, kernel_size=3, strides=strides, padding=\"SAME\"),\n",
        "                            tf.keras.layers.BatchNormalization(),\n",
        "        ]\n",
        "        self.skip_layers = [\n",
        "                            tf.keras.layers.Conv2D(filters, kernel_size=1, strides=strides, padding=\"SAME\"),\n",
        "                            tf.keras.layers.BatchNormalization(),\n",
        "        ]\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)\n"
      ],
      "metadata": {
        "id": "Kk1mdy0lsCwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6DRKuo74ysh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Digit(tf.keras.Model):\n",
        "  def __init__(self,**kwargs):\n",
        "    super(Digit, self).__init__()\n",
        "    self.gen_optimizer = gen_optimizer\n",
        "    self.inputl = tf.keras.layers.InputLayer()\n",
        "    self.hidden1=tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\")\n",
        "    self.hidden2=tf.keras.layers.MaxPool2D(2,2)\n",
        "    self.hidden3=tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\")\n",
        "    self.hidden4=tf.keras.layers.MaxPool2D(2,2)\n",
        "    # self.hidden5=tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\")\n",
        "    # self.hidden6=tf.keras.layers.Dropout(0.5)\n",
        "    # self.hidden7=tf.keras.layers.MaxPool2D(2,2)\n",
        "    self.hidden8=tf.keras.layers.Flatten()\n",
        "    self.hidden9=tf.keras.layers.Dense(64, activation='relu')\n",
        "    self.hidden10=tf.keras.layers.Dense(32, activation='relu')\n",
        "    self.hidden11=tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self,Inputs):\n",
        "      Inputslayer = self.inputl(Inputs)\n",
        "      hidden1 = self.hidden1(Inputslayer)\n",
        "      hidden2 = self.hidden2(hidden1)\n",
        "      hidden3 = self.hidden3(hidden2)\n",
        "      hidden4 = self.hidden4(hidden3)\n",
        "      # hidden5 = self.hidden5(hidden4)\n",
        "      # hidden6 = self.hidden6(hidden5)\n",
        "      # hidden7 = self.hidden7(hidden6)\n",
        "      hidden8 = self.hidden8(hidden4)\n",
        "      hidden9 = self.hidden9(hidden8)\n",
        "      hidden10 = self.hidden10(hidden9)\n",
        "      hidden11 = self.hidden11(hidden10)\n",
        "\n",
        "      return hidden11\n",
        "      \n",
        "  #     return (hidden11)\n",
        "#   def compute_loss(self,Inputs,True_Outputs):\n",
        "#     output = self.call(Inputs)\n",
        "#     output = output.numpy()\n",
        "#     loss = [] \n",
        "    \n",
        "#     for i in range(len(output)):\n",
        "#       k = np.zeros(10)\n",
        "      \n",
        "#       k[True_Outputs[i]] = 1\n",
        "#       output[i] = output[i] - k\n",
        "#       p = output[i]**2\n",
        "#       p = p/10\n",
        "#       loss.append(np.sum(p))\n",
        "#     return loss\n",
        "#   def gradient(self,Inputs,True_Outputs):\n",
        "#     grad = []\n",
        "#     #x = tf.Variable(self.trainable_variables[0])\n",
        "#     print(self.trainable_variables)\n",
        "#     with tf.GradientTape() as tape:\n",
        "#       #tape.watch(x)\n",
        "#       tape.watch(self.trainable_variables)\n",
        "#       loss = self.compute_loss(Inputs,True_Outputs)\n",
        "#       #loss = tf.convert_to_tensor(loss)\n",
        "#     # print(loss[0])\n",
        "#     # print(self.trainable_variables[0].numpy())\n",
        "#     # for i in range(len(loss)):\n",
        "#     gradients = tape.gradient(loss,self.model.trainable_variables)\n",
        "\n",
        "#     # grad.append(gradients)\n",
        "#     print(gradients)\n",
        "    \n",
        "#     # return gradients\n",
        "# def train_weights(model,Inputs,True_Outputs):\n",
        "\n",
        "#   model.trainable_variables = model.trainable_variables - model.gradient(Inputs,True_Outputs)\n",
        "#   return model.trainable_variables\n",
        "  def compute_loss(self,Inputs,True_Outputs):\n",
        "    output = self.call(Inputs)\n",
        "    # print(output)\n",
        "    # output = output.numpy()\n",
        "    # loss = [] \n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    loss = scce( True_Outputs,output)\n",
        "    # for i in range(len(output)):\n",
        "    #   k = np.zeros(10)\n",
        "    #   k[True_Outputs[i]] = 1\n",
        "    #   output[i] = output[i] - k\n",
        "    #   p = output[i]**2\n",
        "    #   p = p/10\n",
        "    #   loss.append(np.sum(p))\n",
        "    return loss\n",
        "  def gradient(self,Inputs,True_Outputs):\n",
        "    \n",
        "    # x = tf.Variable(self.trainable_variables.numpu())\n",
        "    # print(self.trainable_variables[0].numpy())\n",
        "    with tf.GradientTape() as tape:\n",
        "      #tape.watch(x)\n",
        "      tape.watch(self.trainable_variables)\n",
        "      loss = self.compute_loss(Inputs,True_Outputs)\n",
        "      loss = tf.convert_to_tensor(loss)\n",
        "    # print(loss[0])\n",
        "    # print(self.trainable_variables[0].numpy())\n",
        "    # for i in range(len(loss)):\n",
        "    gradients = tape.gradient(loss,self.trainable_variables)\n",
        "    # grad.append(gradients)\n",
        "    return gradients\n",
        "    \n",
        "    # return gradients\n",
        "  def train_weights(self,Inputs,True_Outputs):\n",
        "        gradients = self.gradient(Inputs,True_Outputs)\n",
        "        # print(self.trainable_variables)\n",
        "        # print('hi')\n",
        "        # print('hi')\n",
        "        # print('hi')\n",
        "        # print('hi')\n",
        "\n",
        "        self.gen_optimizer.apply_gradients(zip(gradients, model.trainable_weights))   #applies gradient to change the trainable vaiables i.e w = w - dc_dw\n",
        "        # print(self.trainable_variables)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "3Ht6mW-9t7vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.call(x_train[0].reshape((1,28,28,1))).numpy()[0]"
      ],
      "metadata": {
        "id": "xdvFAHQTtUTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_optimizer = tf.keras.optimizers.Adam(0.001, beta_1=0.5)\n",
        "model = Digit(gen_optimizer = gen_optimizer)\n"
      ],
      "metadata": {
        "id": "bQM6kxsrHeKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_weights(x_train[:1],y_train[:1])"
      ],
      "metadata": {
        "id": "rfTm53PlHnEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.call(x_train[0].reshape((1,28,28,1))).numpy().shape"
      ],
      "metadata": {
        "id": "4_aLI_zztWFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = [\n",
        "    \n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"),\n",
        "    #output shape of above layer is ([1000, 13, 13, 32]). Since  output = (input - kernel_size + 2*padding)/2 + 1 \n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    # tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"),\n",
        "    # tf.keras.layers.MaxPool2D(2,2),\n",
        "    # tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax'),\n",
        "]"
      ],
      "metadata": {
        "id": "0V70A88WN90S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Digit(tf.keras.Model):\n",
        "  def __init__(self,**kwargs):\n",
        "    super(Digit, self).__init__()\n",
        "    self.__dict__.update(kwargs)\n",
        "    self.gen = tf.keras.Sequential(self.gen)\n",
        "    print(self.gen)\n",
        "  def generate(self, z):\n",
        "        return self.gen(z)\n",
        "  def compute_loss(self,Inputs,True_Outputs):\n",
        "    output = self.generate(Inputs)\n",
        "    # print(output)\n",
        "    # output = output.numpy()\n",
        "    # loss = [] \n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    loss = scce( True_Outputs,output)\n",
        "    # for i in range(len(output)):\n",
        "    #   k = np.zeros(10)\n",
        "    #   k[True_Outputs[i]] = 1\n",
        "    #   output[i] = output[i] - k\n",
        "    #   p = output[i]**2\n",
        "    #   p = p/10\n",
        "    #   loss.append(np.sum(p))\n",
        "    return loss\n",
        "  def gradient(self,Inputs,True_Outputs):\n",
        "    \n",
        "    # x = tf.Variable(self.trainable_variables.numpu())\n",
        "    # print(self.trainable_variables[0].numpy())\n",
        "    with tf.GradientTape() as tape:\n",
        "      #tape.watch(x)\n",
        "      tape.watch(self.trainable_variables)\n",
        "      loss = self.compute_loss(Inputs,True_Outputs)\n",
        "      loss = tf.convert_to_tensor(loss)\n",
        "    # print(loss[0])\n",
        "    # print(self.trainable_variables[0].numpy())\n",
        "    # for i in range(len(loss)):\n",
        "    gradients = tape.gradient(loss,self.gen.trainable_variables)\n",
        "    # grad.append(gradients)\n",
        "    return gradients\n",
        "    \n",
        "    # return gradients\n",
        "def train_weights(model,Inputs,True_Outputs):\n",
        "      gradients = model.gradient(Inputs,True_Outputs)\n",
        "      # print(model.trainable_variables)\n",
        "      # print('hi')\n",
        "      # print('hi')\n",
        "      # print('hi')\n",
        "      # print('hi')\n",
        "\n",
        "      model.gen_optimizer.apply_gradients(zip(gradients, model.trainable_weights))   #applies gradient to change the trainable vaiables i.e w = w - dc_dw\n",
        "      # print(model.trainable_variables)\n",
        "def predict(model,Inputs,True_Outputs,test):\n",
        "  train_weights(model,Inputs,True_Outputs)\n",
        "  return model.generate(test)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "ZVdSC3jiLt5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_optimizer = tf.keras.optimizers.Adam(0.001, beta_1=0.5)\n",
        "model = Digit(gen = generator,gen_optimizer = gen_optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijtmeM256LDd",
        "outputId": "ae6b0e54-c380-4fb4-af41-c3ea782167fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.sequential.Sequential object at 0x7f29d05edf10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.array(model.compute_loss(x_train,y_train))\n",
        "model.generate(x_train[:1000])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53pydepxf549",
        "outputId": "3fc38444-354c-4d2d-d850-4e91a72453a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "array([0.08916647, 0.09612306, 0.10065395, 0.08644559, 0.10208869,\n",
              "       0.09388887, 0.1159508 , 0.1150188 , 0.10451461, 0.09614922],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = np.array(model.compute_loss(x_train,y_train))\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiPXjw2Htbge",
        "outputId": "76b4001a-ec36-4d25-a620-1ccc86289c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(2.299929, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k = model.gradient(x_train[:1000],y_train[:1000])"
      ],
      "metadata": {
        "id": "VmmkvI0Zt81R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # for i in range((x_train[:1000].shape[0])-1):\n",
        "  \n",
        "# #   train_weights(model,x_train[i:i+1],y_train[i:i+1])\n",
        "# k = predict(model,x_train,y_train,x_test[1:2]).numpy()\n",
        "# p = max(k[0])\n",
        "# for i in range(len(k[0])):\n",
        "#   if k[0][i] == p:\n",
        "#     print(i)\n",
        "\n",
        "# y_test[1:2]"
      ],
      "metadata": {
        "id": "4G35Fh-gFCqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range((x_train[:500].shape[0])-1):\n",
        "  train_weights(model,x_train[i:i+1],y_train[i:i+1])\n",
        "  loss = np.array(model.compute_loss(x_train,y_train))\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNQffcnqcRAw",
        "outputId": "db2f1420-a3f8-436e-f3b2-1a8ed1e00214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1778314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = np.array(model.compute_loss(x_train,y_train))\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR8Mwnnjb0rQ",
        "outputId": "41ed1d92-0a8a-4798-bcf3-4655ed5e555a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(1.1778314, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k = predict(model,x_train,y_train,x_test[0:1]).numpy()\n",
        "# loss = np.array(model.compute_loss(x_train,y_train))\n",
        "# print(loss)\n",
        "# p = max(k[0])\n",
        "# for i in range(len(k[0])):\n",
        "#   if k[0][i] == p:\n",
        "#     print(i)\n",
        "\n",
        "# y_test[0:1]"
      ],
      "metadata": {
        "id": "wpRGDYy1eCrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "var =100\n",
        "for i in range(len(x_test[:var])-1):\n",
        "  k = predict(model,x_train,y_train,x_test[i:i+1]).numpy()\n",
        "  p = max(k[0])\n",
        "  for j in range(len(k[0])):\n",
        "    if k[0][j] == p:\n",
        "      output = j\n",
        "  # print(output)\n",
        "  # print(y_test[i:i+1])\n",
        "  if output == y_test[i:i+1]:\n",
        "    count = count +1 \n",
        "print(100*count/(len(x_test[:var])-1))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuhYGuyEFNLL",
        "outputId": "679af2e6-a251-4ab9-e15e-52a3776cfd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96.96969696969697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GgmjXP5XRjn7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}